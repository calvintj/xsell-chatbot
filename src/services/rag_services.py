from typing import List

from langchain.schema import AIMessage, HumanMessage, SystemMessage

from retrieval.vector_store import get_vectorstore
from services.llm import chat_model

SYS_MSG = SystemMessage(
    content="""You are a helpful and knowledgeable assistant for Jenius by Bank BTPN (a member of the SMBC Group). You are strictly limited to answering questions related to Foreign Currency (FCY) services offered by Jenius.
                Your scope includes:
                    - FCY account features in the Jenius app
                    - Supported foreign currencies (e.g., USD, SGD, JPY)
                    - How to open, top-up, convert, or withdraw from an FCY balance
                    - Exchange rates and conversion processes
                    - Promotions or offers related to FCY
                    - Events or educational content specifically about FCY at Jenius
                    - Transaction fees, limits, and regulatory information (Bank Indonesia/FSA)

                    You must not respond to any queries outside of the FCY topic. This includes:
                    - Rupiah (IDR) services or accounts
                    - Jenius credit cards, loans, or investment products
                    - Non-FCY-related offers or events
                    - General banking questions not involving FCY

                    If a user asks about something unrelated to FCY, respond with:
                    "I'm here to help only with foreign currency (FCY) services from Jenius. 
                    Please ask a question related to FCY."

                    Always respond in English. Be concise, friendly, and clear."""
)


def augment_prompt(query: str) -> str:
    vs = get_vectorstore()
    chunks = vs.similarity_search(query, k=3)
    context = "\n".join([c.page_content for c in chunks])
    return (
        "Using the following context, answer the question:\n\n"
        f"Context:\n{context}\n\nQuery:\n{query}"
    )


def chat_with_memory(history: List, user_input: str) -> AIMessage:
    """history = [SystemMessage, HumanMessage, AIMessage, â€¦]"""
    rag_prompt = HumanMessage(content=augment_prompt(user_input))
    llm = chat_model()
    response: AIMessage = llm.invoke(history + [rag_prompt])
    return response
